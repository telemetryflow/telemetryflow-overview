# Shared Module: Cache (Multi-Level Caching)

- **Version:** 1.0.0-CE
- **Last Updated:** December 12, 2025
- **Status:** ‚úÖ Production Ready
- **Priority:** üî• HIGH - Performance Critical

---

## Module Overview

```mermaid
graph TB
    subgraph "Cache Module"
        A[L1 Cache<br/>In-Memory Map]
        B[L2 Cache<br/>Redis]
        C[Cache Service<br/>Unified API]
        D[Cache Invalidation<br/>Pattern-based]
    end

    subgraph "Key Features"
        E[‚úÖ Two-Level Caching]
        F[‚úÖ Automatic Failover]
        G[‚úÖ TTL Management]
        H[‚úÖ Pattern Invalidation]
    end

    A --> E
    B --> F
    C --> G
    D --> H

    style A fill:#ff6b6b
    style B fill:#4ecdc4
    style C fill:#45b7d1
    style D fill:#f9ca24
```

**Purpose:** High-performance multi-level caching system with L1 (in-memory) and L2 (Redis) tiers for optimal latency and distributed caching.

**Location:** `backend/src/shared/cache/`

---

## Architecture

### Cache Layers

```mermaid
graph TB
    subgraph "Application Layer"
        APP[Application Request]
    end

    subgraph "L1 Cache - In-Memory"
        L1[Node.js Map<br/>Max: 1000 items<br/>TTL: 60s]
    end

    subgraph "L2 Cache - Redis"
        L2[Redis<br/>Distributed<br/>TTL: 30min]
    end

    subgraph "Data Source"
        DB[PostgreSQL/ClickHouse]
    end

    APP -->|1. Check L1| L1
    L1 -->|Hit| APP
    L1 -->|Miss| L2
    L2 -->|Hit| L1
    L2 -->|Hit| APP
    L2 -->|Miss| DB
    DB --> L2
    DB --> L1
    DB --> APP

    style L1 fill:#27ae60
    style L2 fill:#4ecdc4
    style DB fill:#6c5ce7
```

### Cache Service Architecture

```mermaid
classDiagram
    class CacheService {
        -l1Cache: Map
        -redisClient: Redis
        -l1MaxSize: 1000
        -l1DefaultTTL: 60000ms
        -l2DefaultTTL: 1800s
        +get(key) Promise~T~
        +set(key, value, ttl) Promise~void~
        +del(key) Promise~void~
        +delPattern(pattern) Promise~number~
        +clear() Promise~void~
        +getStats() Promise~Stats~
        +wrap(key, fn, ttl) Promise~T~
    }

    class CacheKeyGenerator {
        +metrics(tenantId, metricName, timeRange) string
        +logs(tenantId, query, page) string
        +dashboard(tenantId, dashboardId) string
        +services(tenantId) string
        +pattern(prefix, tenantId) string
    }

    class CacheConfig {
        +l1TTL number
        +l1Max number
        +l2TTL number
        +l2Host string
        +l2Port number
        +l2DB number
        +keyPatterns object
    }

    CacheService --> CacheKeyGenerator
    CacheService --> CacheConfig

    style CacheService fill:#4ecdc4
    style CacheKeyGenerator fill:#f9ca24
```

---

## Cache Flow

### Complete Cache Operation

```mermaid
sequenceDiagram
    participant App as Application
    participant Svc as CacheService
    participant L1 as L1 (Map)
    participant L2 as Redis
    participant DB as Database

    App->>Svc: get("metrics:tenant1:cpu:1h")

    Svc->>L1: Check L1 cache
    alt L1 Hit
        L1-->>Svc: Return cached data
        Svc-->>App: Data (10ms) ‚ö°
    else L1 Miss
        Svc->>L2: Check L2 cache (Redis)

        alt L2 Hit
            L2-->>Svc: Return cached data
            Svc->>L1: Store in L1 (write-through)
            Svc-->>App: Data (50ms) üü¢
        else L2 Miss
            Svc->>DB: Query database
            DB-->>Svc: Return data

            par Store in both levels
                Svc->>L1: Set in L1 (TTL: 60s)
                Svc->>L2: Set in L2 (TTL: 30min)
            end

            Svc-->>App: Data (200ms) üü°
        end
    end

    Note over App,DB: L1: 10ms | L2: 50ms | DB: 200ms
```

### Cache Invalidation Flow

```mermaid
sequenceDiagram
    participant App as Application
    participant Svc as CacheService
    participant L1 as L1 (Map)
    participant L2 as Redis

    App->>Svc: delPattern("metrics:tenant1:*")

    par L1 Invalidation
        Svc->>L1: Iterate Map entries
        L1->>L1: Match pattern with regex
        L1->>L1: Delete matched keys
        L1-->>Svc: Count: 45 deleted
    and L2 Invalidation
        Svc->>L2: SCAN with pattern
        L2-->>Svc: Matching keys (batches of 100)
        Svc->>L2: DEL keys (batch delete)
        L2-->>Svc: Count: 123 deleted
    end

    Svc-->>App: Total: 168 keys deleted
```

---

## Implementation

### Cache Service Code

```typescript
// cache.service.ts
@Injectable()
export class CacheService implements OnModuleInit {
  // L1 Cache: In-memory Map
  private l1Cache: Map<string, { value: any; expiry: number }> = new Map();
  private readonly l1MaxSize: number = 1000;
  private readonly l1DefaultTTL: number = 60 * 1000; // 60 seconds

  // L2 Cache: Redis client
  private redisClient: Redis;
  private readonly l2DefaultTTL: number = 30 * 60; // 30 minutes in seconds

  async onModuleInit() {
    // Initialize Redis asynchronously (non-blocking)
    setImmediate(async () => {
      try {
        this.redisClient = new Redis({
          host: process.env.REDIS_HOST || 'localhost',
          port: parseInt(process.env.REDIS_PORT || '6379'),
          password: process.env.REDIS_PASSWORD || undefined,
          db: parseInt(process.env.REDIS_CACHE_DB || '1'),
          connectTimeout: 5000,
          maxRetriesPerRequest: 1,
          enableOfflineQueue: false,
          lazyConnect: true,
        });

        await this.redisClient.connect();
        this.logger.log('‚úì Cache service initialized with L1 + L2 (Redis)');
      } catch (error) {
        this.logger.warn('‚ö† Redis unavailable, operating in L1-only mode');
        this.redisClient = null;
      }
    });

    // Start L1 cleanup interval (every minute)
    setInterval(() => this.cleanupL1Cache(), 60 * 1000);
  }

  /**
   * Get value from cache (L1 ‚Üí L2 ‚Üí null)
   */
  async get<T>(key: string): Promise<T | null> {
    // Try L1 first
    const l1Result = this.getFromL1<T>(key);
    if (l1Result !== null) {
      this.logger.debug(`L1 cache HIT: ${key}`);
      return l1Result;
    }

    // Try L2 (Redis)
    const l2Result = await this.getFromL2<T>(key);
    if (l2Result !== null) {
      this.logger.debug(`L2 cache HIT: ${key}`);
      // Store in L1 for faster subsequent access
      this.setToL1(key, l2Result, this.l1DefaultTTL);
      return l2Result;
    }

    this.logger.debug(`Cache MISS: ${key}`);
    return null;
  }

  /**
   * Set value in cache (both L1 and L2)
   */
  async set<T>(key: string, value: T, ttl?: number): Promise<void> {
    const l1TTL = ttl || this.l1DefaultTTL;
    const l2TTL = ttl ? Math.floor(ttl / 1000) : this.l2DefaultTTL;

    // Set in both levels
    this.setToL1(key, value, l1TTL);
    await this.setToL2(key, value, l2TTL);

    this.logger.debug(`Cache SET: ${key} (L1: ${l1TTL}ms, L2: ${l2TTL}s)`);
  }

  /**
   * Delete value from cache (both L1 and L2)
   */
  async del(key: string): Promise<void> {
    this.l1Cache.delete(key);
    if (this.redisClient) {
      await this.redisClient.del(key);
    }
  }

  /**
   * Wrap a function with caching
   */
  async wrap<T>(
    key: string,
    fn: () => Promise<T>,
    ttl?: number,
  ): Promise<T> {
    // Try to get from cache
    const cached = await this.get<T>(key);
    if (cached !== null) {
      return cached;
    }

    // Execute function
    const result = await fn();

    // Store in cache
    await this.set(key, result, ttl);

    return result;
  }

  // Private methods for L1 and L2 operations...
}
```

### Cache Key Generator

```typescript
// cache.config.ts
export class CacheKeyGenerator {
  /**
   * Generate cache key for metrics
   */
  static metrics(tenantId: string, metricName: string, timeRange: string): string {
    return `metrics:${tenantId}:${metricName}:${timeRange}`;
  }

  /**
   * Generate cache key for logs
   */
  static logs(tenantId: string, query: string, page: number): string {
    const queryHash = Buffer.from(query).toString('base64').substring(0, 20);
    return `logs:${tenantId}:${queryHash}:${page}`;
  }

  /**
   * Generate cache key for dashboard
   */
  static dashboard(tenantId: string, dashboardId: string): string {
    return `dashboard:${tenantId}:${dashboardId}`;
  }

  /**
   * Generate pattern for cache invalidation
   */
  static pattern(prefix: string, tenantId?: string): string {
    if (tenantId) {
      return `${prefix}:${tenantId}:*`;
    }
    return `${prefix}:*`;
  }
}
```

---

## Cache TTL Strategy

```mermaid
graph TB
    subgraph "TTL Tiers"
        A[Short-lived<br/>1 min / 5 min]
        B[Medium-lived<br/>5 min / 30 min]
        C[Long-lived<br/>30 min / 1 hour]
        D[Static<br/>1 hour / 24 hours]
    end

    subgraph "Data Types"
        E[Real-time Metrics]
        F[Dashboard Data]
        G[Service Lists]
        H[Templates]
    end

    E --> A
    F --> B
    G --> C
    H --> D

    style A fill:#e74c3c
    style B fill:#f39c12
    style C fill:#27ae60
    style D fill:#3498db
```

| Data Type | L1 TTL | L2 TTL | Rationale |
|-----------|--------|--------|-----------|
| **Real-time Metrics** | 1 min | 5 min | Frequently changing data |
| **Dashboard Data** | 5 min | 30 min | Moderate update frequency |
| **Service Lists** | 30 min | 1 hour | Relatively static |
| **Templates** | 1 hour | 24 hours | Rarely changes |
| **Session Data** | 5 min | 15 min | User activity-based |
| **Permission Cache** | 15 min | 1 hour | Security-sensitive |

---

## Cache Configuration

### Environment Variables

```bash
# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_CACHE_DB=1  # Separate DB for cache

# L1 Cache Configuration
CACHE_L1_MAX_SIZE=1000
CACHE_L1_DEFAULT_TTL=60000  # milliseconds

# L2 Cache Configuration
CACHE_L2_DEFAULT_TTL=1800  # seconds
```

### Default Configuration

```typescript
export const defaultCacheConfig: CacheConfig = {
  // L1: In-Memory Cache (fast, short TTL)
  l1: {
    ttl: 60 * 1000, // 60 seconds
    max: 1000, // Maximum 1000 items
  },

  // L2: Redis Cache (distributed, longer TTL)
  l2: {
    ttl: 30 * 60 * 1000, // 30 minutes
    host: process.env.REDIS_HOST || 'localhost',
    port: parseInt(process.env.REDIS_PORT || '6379', 10),
    password: process.env.REDIS_PASSWORD || undefined,
    db: parseInt(process.env.REDIS_CACHE_DB || '1', 10),
  },

  // Cache key patterns
  keyPatterns: {
    metrics: 'metrics:{tenantId}:{metricName}:{timeRange}',
    logs: 'logs:{tenantId}:{query}:{page}',
    dashboard: 'dashboard:{tenantId}:{dashboardId}',
    session: 'session:{userId}',
    realtime: 'realtime:{tenantId}:{metricName}',
  },
};
```

---

## Usage Examples

### Basic Usage

```typescript
// Get from cache
const data = await cacheService.get<MetricData>('metrics:tenant1:cpu:1h');

// Set in cache
await cacheService.set('metrics:tenant1:cpu:1h', metricData, 60000);

// Delete from cache
await cacheService.del('metrics:tenant1:cpu:1h');

// Pattern-based deletion
await cacheService.delPattern('metrics:tenant1:*');
```

### Wrap Function with Caching

```typescript
// Automatically cache function result
const metricData = await cacheService.wrap(
  'metrics:tenant1:cpu:1h',
  async () => {
    // Expensive database query
    return await clickhouse.query('SELECT ...');
  },
  300000 // 5 minutes TTL
);
```

### Using Cache Decorator

```typescript
@Injectable()
export class MetricsService {
  @Cacheable('metrics:{tenantId}:{metricName}:{timeRange}', 300000)
  async getMetricTimeSeries(
    tenantId: string,
    metricName: string,
    timeRange: string
  ): Promise<MetricData> {
    // This result will be automatically cached
    return await this.repository.findTimeSeries(...);
  }
}
```

---

## Performance Metrics

### Cache Hit Rates

```mermaid
pie title Cache Hit Distribution
    "L1 Hit (10ms)" : 60
    "L2 Hit (50ms)" : 30
    "Miss (200ms)" : 10
```

| Metric | Target | Actual | Performance |
|--------|--------|--------|-------------|
| **L1 Hit Rate** | > 50% | 60% | ‚úÖ Excellent |
| **L2 Hit Rate** | > 30% | 30% | ‚úÖ Good |
| **Combined Hit Rate** | > 80% | 90% | ‚úÖ Excellent |
| **L1 Latency** | < 10ms | 5ms | ‚úÖ Excellent |
| **L2 Latency** | < 100ms | 50ms | ‚úÖ Excellent |

### Performance Comparison

| Operation | No Cache | L2 Only | L1 + L2 | Improvement |
|-----------|----------|---------|---------|-------------|
| **Metric Query** | 200ms | 50ms | 10ms | 20x faster |
| **Dashboard Load** | 500ms | 150ms | 30ms | 16x faster |
| **Service List** | 100ms | 40ms | 5ms | 20x faster |

---

## Monitoring

### Cache Statistics

```typescript
// Get cache statistics
const stats = await cacheService.getStats();

// Response:
{
  l1: {
    size: 567,
    maxSize: 1000,
    hitRate: 0.62
  },
  l2: {
    keys: 12453,
    memory: "45.2 MB",
    hitRate: 0.31
  }
}
```

### Cache Admin Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/admin/cache/stats` | GET | Get cache statistics |
| `/admin/cache/clear` | POST | Clear all cache |
| `/admin/cache/clear/:pattern` | POST | Clear by pattern |

---

## Best Practices

### When to Use Cache

‚úÖ **Good Use Cases:**
- Frequently queried, rarely changed data
- Expensive database queries
- API responses with high request rate
- Session data
- Permission lookups

‚ùå **Avoid Caching:**
- Real-time data requiring millisecond freshness
- User-specific sensitive data (unless encrypted)
- Data that changes on every request
- Very large objects (> 1 MB)

### Cache Key Design

```typescript
// ‚úÖ Good: Hierarchical, specific
CacheKeyGenerator.metrics('tenant-123', 'cpu_usage', '1h')
// ‚Üí "metrics:tenant-123:cpu_usage:1h"

// ‚úÖ Good: Supports pattern invalidation
CacheKeyGenerator.pattern('metrics', 'tenant-123')
// ‚Üí "metrics:tenant-123:*"

// ‚ùå Bad: No hierarchy, hard to invalidate
'tenant-123-cpu_usage-1h'
```

---

## Related Modules

```mermaid
graph TB
    C[shared/cache<br/>Cache Service]

    C -->|Used by| M[400-telemetry<br/>Metrics Caching]
    C -->|Used by| D[900-dashboard<br/>Dashboard Caching]
    C -->|Used by| A[200-auth<br/>Session Caching]
    C -->|Used by| I[100-core<br/>Permission Caching]

    style C fill:#4ecdc4
    style M fill:#ff6b6b
    style D fill:#f9ca24
```

---

## Testing

### Unit Tests
- `CacheService.spec.ts` - Service logic
- `CacheKeyGenerator.spec.ts` - Key generation

### Integration Tests
- `cache-integration.spec.ts` - Redis integration

---

- **File Location:** `./backend/shared/cache.md`
- **Maintained By:** DevOpsCorner Indonesia
